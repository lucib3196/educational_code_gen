{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZWn2w18H0eVhB2vNYcxUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucib3196/educational_code_gen/blob/main/Introduction_to_Large_Language_Models_(LLMs)_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring Large Language Models with OpenAI\n",
        "In this introductory notebook, well go over the capabilities and applications of AI models available today. LLMs, such as GPT (Generative Pre-trained Transformer), which allows us to generate content via natural language processing.  \n",
        "\n",
        "Our focus will be on practicality and getting up and running with OpenAI's API, understanding the basics of interacting with LLMs, and exploring the foundational concept of prompt engineering\n",
        "\n",
        "**Topic Covered**\n",
        "- The basics of Large Language Models\n",
        "- How to securely set up and use the OpenAI API in your projects\n",
        "- Simple strategies for crafting effective prompts to communicate with LLMs."
      ],
      "metadata": {
        "id": "OmZm14cst4td"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up"
      ],
      "metadata": {
        "id": "CDjsObOysAPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVOT4Lfnpn81",
        "outputId": "33d9e64a-0b42-4487-ce15-a1a486b22842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/226.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.12.0\n"
          ]
        }
      ],
      "source": [
        "# Install Libraries\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from getpass import getpass\n"
      ],
      "metadata": {
        "id": "I6rgEoC9rAxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert Personal API key\n",
        "api_key = getpass('Enter your OpenAI API Key: ')\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "Q2anfeR3rL_6",
        "outputId": "ada625aa-03b8-434b-f086-ef1991e09be0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to API Calls with OpenAI's GPT Models\n",
        "\n",
        "In this section, we'll explore the fundamental structure of making an API call to interact with Large Language Models (LLMs) like GPT-3.5 or GPT-4.\n",
        "\n",
        " An API call to these models  involves specifying a model and crafting messages that guide the interaction. Here's a breakdown:\n",
        "\n",
        "**Model**: This refers to the specific version of the LLM you wish to use, such as GPT-3.5 or GPT-4. Each version has its strengths, and your choice may depend on the complexity of the tasks you aim to perform.\n",
        "\n",
        "**Messages**: This is an array of messages that direct the flow of conversation between you (the user) and the model. Each message in the array has a defined role, which can be one of the following:\n",
        "\n",
        "**system**: Specifies instructions or constraints on how the model should behave during the interaction. This could include directives on tone, style, or specific rules to follow.\n",
        "\n",
        "**user**: Represents messages from the user, containing the requests or prompts for the LLM to respond to.\n",
        "\n",
        "\n",
        "In our example, we will set up a simple scenario where the goal is to generate a haiku about a dog."
      ],
      "metadata": {
        "id": "K6eO6-UMvKrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a poem creating assistant, who specialized in haiku creation.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a haiku about a dog\"}\n",
        "  ]\n",
        ")"
      ],
      "metadata": {
        "id": "kWUNLYdivNiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "lGtyd59DvR0p",
        "outputId": "2d2578b0-7c3f-416a-90ba-dc37bc0a2821",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A loyal friend dear,\n",
            "Boundless joy in every tail,\n",
            "Forever by my side.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incorporating Inputs in Predefined Prompts\n",
        "Utilizing individual messages is effective, but incorporating new values into predefined prompts elevates our interaction with the model, ensuring consistency in structure and theme.\n",
        "\n",
        "Take, for instance, our haiku generation template. By designing a base template for haikus, we can effortlessly introduce different subjects into the same structure, enabling the creation of unique and themed haikus with minimal adjustments. This approach showcases prompt engineering, allowing for versatile applications while maintaining a coherent framework."
      ],
      "metadata": {
        "id": "DlD0rTEWyjYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "haiku_subjects = [\"dog\", \"UCR\", \"cereal\", \"math\"]\n",
        "\n",
        "for subject in haiku_subjects:\n",
        "  haiku_prompt = f\"Compose a haiku about a {subject}\"\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a poem creating assistant, who specialized in haiku creation.\"},\n",
        "      {\"role\": \"user\", \"content\":haiku_prompt }\n",
        "    ]\n",
        "  )\n",
        "  generated_haiku = completion.choices[0].message.content\n",
        "  print(f\"The subject of the haiku is: {subject}\\n \" +  generated_haiku + \"\\n\"  )"
      ],
      "metadata": {
        "id": "lxh-6jAm2R6V",
        "outputId": "ad08b68e-d6b5-4172-a6c0-50f69f7d2ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The subject of the haiku is: dog\n",
            " Loyal friend in fur,\n",
            "Tail wagging, full of love, joy,\n",
            "Paws padding softly.\n",
            "\n",
            "The subject of the haiku is: UCR\n",
            " Branches reaching high,\n",
            "Campus buzzing with knowledge,\n",
            "UCR shines bright.\n",
            "\n",
            "The subject of the haiku is: cereal\n",
            " Crunchy grains aglow,\n",
            "Milk dance in a golden sea,\n",
            "Morning melody.\n",
            "\n",
            "The subject of the haiku is: math\n",
            " Numbers dance and play\n",
            "In equations, patterns sway\n",
            "Mathematics' way\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Engineering: The Importance of Specificity\n",
        "Prompt engineering is crucial for directing Large Language Models (LLMs) to produce desired outputs. Vague prompts often result in off-target responses due to the model's broad interpretation.\n",
        "\n",
        "Avoiding Vagueness\n",
        "Vague prompts like \"Write a story\" can lead to generic and unrelated outputs. Being specific narrows the model's focus, ensuring outputs align more closely with your intentions.\n",
        "\n",
        "Being Specific\n",
        "Incorporate clear, detailed instructions to guide the model effectively. For instance, \"Write a suspenseful story set in a deserted mountain village, involving a lost treasure\" yields more precise and relevant responses.\n"
      ],
      "metadata": {
        "id": "9jUeegSw3cLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "generic_prompt = \"Write a short poem \"\n",
        "specific_propmpt = \"Write a short and suspensful poem about me preparing for finals\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": generic_prompt}\n",
        "  ]\n",
        ")\n",
        "generic_poem = completion.choices[0].message.content\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": specific_propmpt}\n",
        "  ]\n",
        ")\n",
        "specific_poem =completion.choices[0].message.content\n",
        "\n",
        "\n",
        "print(\"Generic Peom : \"+  generic_poem + \"\\n\" + \"----------------------------------------\")\n",
        "print(\"Specific Poem : \" + specific_poem+\" \\n\")"
      ],
      "metadata": {
        "id": "ZBcwhxlH4TCR",
        "outputId": "f33468d2-49f3-497a-a00b-73870d292af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generic Peom : In the whispering breeze, secrets unfold,\n",
            "Stories untold in the sunset's gold.\n",
            "In the hush of night, stars softly gleam,\n",
            "Dreams dance in a moonbeam stream.\n",
            "\n",
            "In the silence of dawn, birds take flight,\n",
            "Hope and promise in morning's light.\n",
            "In the embrace of love, hearts entwine,\n",
            "Forever connected, a bond divine.\n",
            "\n",
            "Through life's ebb and flow, we find our way,\n",
            "Guided by love and light every day.\n",
            "In beauty and grace, the world's charm,\n",
            "A tapestry of life, held in nature's calm.\n",
            "----------------------------------------\n",
            "Specific Poem : In shadows cast by flickering light,\n",
            "A student toils through the night.\n",
            "Pages turning, mind ablaze,\n",
            "Preparation for the final maze.\n",
            "\n",
            "Heart racing, with time slipping away,\n",
            "Each word studied, each point gained.\n",
            "The pressure mounts, the stakes are high,\n",
            "As the clock ticks on with a silent cry.\n",
            "\n",
            "Whispers of doubt, fears unspoken,\n",
            "In the stillness of the night, your spirit broken.\n",
            "But in the darkness, a spark ignites,\n",
            "A glimmer of hope, shining bright.\n",
            "\n",
            "With one last breath, you take a stand,\n",
            "Armed with knowledge, at your command.\n",
            "Finals looming, fate unknown,\n",
            "But in your strength, your worth is shown. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shot Prompting: Enhancing Model Performance\n",
        "\n",
        "Shot prompting is a technique that improves model performance by providing examples of the desired output format or content. This method guides the model more effectively, showcasing how to structure responses or tackle specific tasks.\n",
        "\n",
        "### Why Use Shot Prompting?\n",
        "By including examples, you directly inform the model of your expectations, leading to more accurate and contextually relevant outputs. This is particularly useful for complex requests or when seeking consistency in the model's responses.\n",
        "\n",
        "### Improving Performance\n",
        "Shot prompting can significantly enhance the model's ability to understand and execute tasks by demonstrating the exact format or approach desired. Whether it's \"few-shot\" (providing a few examples) or \"zero-shot\" (providing a description of the task without examples), the clarity and detail in your prompts directly influence the model's output quality.\n",
        "\n",
        "\n",
        "In the example below, I aim for the model to identify a profession based on a specific skill.\n",
        "\n",
        "Example:\n",
        "\n",
        "- math -> mathematician\n",
        "- cooking -> chef\n",
        "- fitness -> coach\n",
        "\n",
        "For the first approach, I'll use a descriptive prompt, whereas for the second, I'll provide a clear example."
      ],
      "metadata": {
        "id": "N-A9PP6w5yRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "skills = [\"good with animals\", \"enjoy physics and building stuff\", \"good at making coffee\"]\n",
        "\n",
        "for skill in skills:\n",
        "  no_example_prompt = f\"You are tasked to identify a profession based on a specific skill, give me the one that comes first to mind{skill} \"\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": no_example_prompt}\n",
        "    ]\n",
        "  )\n",
        "  no_example_output = completion.choices[0].message.content\n",
        "\n",
        "\n",
        "  example_prompt = f\"\"\"\n",
        "  identify a profession based on a specific skill.Give me the one that comes first to mind\n",
        "  The output should follow the following format\n",
        "  input: math\n",
        "  output: mathematician\n",
        "  input: cooking\n",
        "  output: chef\n",
        "  input:fitness\n",
        "  output: coach\n",
        "\n",
        "  input{skill}\n",
        "  only return output\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": example_prompt}\n",
        "    ]\n",
        "  )\n",
        "  example_output =completion.choices[0].message.content\n",
        "\n",
        "\n",
        "  print(\"No Examples : \"+  no_example_output + \"\\n\" + \"----------------------------------------\")\n",
        "  print(\"With Examples : \" + example_output+\" \\n\")"
      ],
      "metadata": {
        "id": "YMrwiZRa6U0D",
        "outputId": "b5d2f7f4-761d-450c-f0b1-02bbbc6b93a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Examples : The profession that comes to mind based on being good with animals is a veterinarian.\n",
            "----------------------------------------\n",
            "With Examples : veterinarian \n",
            "\n",
            "No Examples : The profession that comes to mind based on enjoying physics and building stuff is a structural engineer. Structural engineers use principles of physics and mathematics to design and analyze buildings, bridges, and other structures to ensure they are safe and can withstand various forces and loads. They play a crucial role in the construction industry by creating sound structural designs that meet building codes and regulations.\n",
            "----------------------------------------\n",
            "With Examples : engineer \n",
            "\n",
            "No Examples : Barista\n",
            "----------------------------------------\n",
            "With Examples : barista \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using just examples is not ideal in reality When using shot prompting, aim to combine descriptive instructions with specific examples to achieve more consistent and accurate outputs."
      ],
      "metadata": {
        "id": "i44781aA8yxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Output\n",
        "When managing outputs, it's beneficial to organize them in a structured format, with one of the simplest and most effective methods being a JSON object."
      ],
      "metadata": {
        "id": "z7Zlcpuo9aep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-0125\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020? Return in json structure with the key being 'winner'\"}\n",
        "  ]\n",
        ")\n",
        "output = response.choices[0].message.content\n",
        "print(\"LLM Output : \"+output+\"\\n\")\n",
        "python_dict = json.loads(output)\n",
        "\n",
        "print(\"The output as a dictonary: \"+str(python_dict)+\" \\n\")\n",
        "print(\"Parsing Dictionary \" + python_dict[\"winner\"]+ \"\\n\")"
      ],
      "metadata": {
        "id": "Oo0OlnuF6YrM",
        "outputId": "3ce011da-3012-45b8-935f-ef876451c390",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output : {\n",
            "    \"winner\": \"Los Angeles Dodgers\"\n",
            "}\n",
            "\n",
            "The output as a dictonary: {'winner': 'Los Angeles Dodgers'} \n",
            "\n",
            "Parsing Dictionary Los Angeles Dodgers\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sources\n",
        "- https://platform.openai.com/docs/guides/text-generation"
      ],
      "metadata": {
        "id": "sgdSF8k-_MIZ"
      }
    }
  ]
}